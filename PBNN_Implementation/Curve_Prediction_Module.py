## This code implements the code to represent  ##

import tensorflow as tf
tf.compat.v1.enable_eager_execution()
import os
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from time import time
import pandas as pd
import re
import scipy

from scipy.interpolate import interp1d

#%% SECTION TO RUN WITH GPU

# Choose GPU to use
os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID";

# The GPU ID to use, usually either "0" or "1"
os.environ["CUDA_VISIBLE_DEVICES"]="0";

Config=tf.compat.v1.ConfigProto(allow_soft_placement=True)
Config.gpu_options.allow_growth=True

#%% Define Parameters

boxW = 0.25 + 0.025*2
boxL = 0.25 + 0.025*2
boxH = 1

def Ogden_func(x, w):
    
    p1 = 2*w[0]/w[3] * (x**(w[3]-1) - x**(-0.5*w[3]-1))
    p2 = 2*w[1]/w[4] * (x**(w[4]-1) - x**(-0.5*w[4]-1))
    p3 = 2*w[2]/w[5] * (x**(w[5]-1) - x**(-0.5*w[5]-1))
    
    return p1 + p2 + p3

#%% Load stress-strain curve

strain_set = np.linspace(0,1,21) * 0.15
lambda_set = strain_set + 1

ss_set = np.load('odgen_params.npy')

# Load Syntactic Foam models
label_set = np.load('syntac_foam_new.npy')  

# Pick the models solved by Numerical Solver
label_set = label_set[0:6925]

#%% Image Visualization for validation
#aaa = label_set[0]
#plt.imshow(aaa)

#%% Load back to original (As the input data are all taken absolution value in the data preprocessing for testing purpose)
ss_set[:,2] = -ss_set[:,2]
ss_set[:,4] = -ss_set[:,4]

#%% Load existing model (This model is generated by Feature_Extraction_Module)
cnn_model = tf.keras.models.load_model('conv_model_128_good.h5')

#%% Predefine Function
def Ogden_func(w):
    
    x=1.15
    
    p1 = 2*w[0]/w[3] * (x**(w[3]-1) - x**(-0.5*w[3]-1))
    p2 = 2*w[1]/w[4] * (x**(w[4]-1) - x**(-0.5*w[4]-1))
    p3 = 2*w[2]/w[5] * (x**(w[5]-1) - x**(-0.5*w[5]-1))
    
    return p1 + p2 + p3

#%% Define Function Terms
[p1,p2] = ss_set.shape

end_term = np.zeros((p1,1))

lam = 1.15

for i in range(p1):
    # slope_term[i][0] = ss_set[i][0] + ss_set[i][1] + ss_set[i][2]
    end_term[i][0] = Ogden_func(ss_set[i,:])

#%% Data Concatenation
ss_set_all = np.concatenate((ss_set,end_term),axis=1)

#%% Data Splitting
l_s, l_x, l_y = label_set.shape

X_train, X_test, Y_train, Y_test = train_test_split(label_set, ss_set_all, test_size=0.4, random_state=47)
X_test,  X_cv,   Y_test,  Y_cv   = train_test_split(X_test,    Y_test,       test_size=0.5, random_state=47)

#%% Define Convolutional Network Functions
def dense_tanh_drop_block(x,filt,names):
    
    y = tf.keras.layers.Dense(filt, activation='linear', use_bias=True,
                              name=names)(x)   
    y = tf.keras.activations.tanh(y)   
    y = tf.keras.layers.Dropout(0.2)(y)
    
    return y

def dense_block(x,filt):
    
    y = tf.keras.layers.Dense(filt, activation='linear', use_bias=True)(x)
    
    return y

def dense_selu_block(x,filt,names):
    
    y = tf.keras.layers.Dense(filt, kernal_initializer='lecun_normal', 
                              activation='selu', use_bias=True,
                              name=names)(x)
    
    return y

def dense_relu_block(x,filt,names):
    
    y = tf.keras.layers.Dense(filt, activation='linear', use_bias=True,
                              name=names)(x)
    
    y = tf.keras.layers.ReLU()(y)

    return y

def dense_tanh_block(x,filt,names):
    
    y = tf.keras.layers.Dense(filt, activation='linear', use_bias=True,
                              name=names)(x)
    
    y = tf.keras.activations.tanh(y)
    
    #y = tf.keras.layers.Dropout(0.5)(y)
    
    return y

#%% Define the Neural Network

input_layer = tf.keras.Input(shape=(128))

dense_1 = dense_tanh_block(input_layer, 64, 'dense_01')
dense_2 = dense_tanh_block(dense_1,     64, 'dense_02')
# dense_3 = dense_tanh_block(dense_2,      64, 'dense_03')
# dense_4 = dense_tanh_block(dense_3,      64, 'dense_04')

output_layer = dense_block(dense_2, 7)
dense_model = tf.keras.models.Model(inputs=input_layer, outputs=output_layer)

dense_model.summary()

#%% Setting up whole framework
def define_gan(g_model, d_model):
    
    g_model.trainable = False
    
    input_1 = tf.keras.Input(shape=(l_x, l_y, 1))

    inter_output = g_model(input_1)
    feature      = tf.reshape(inter_output, [-1,128])
    
    output    = d_model(feature)
    
    gan_model = tf.keras.models.Model(inputs=input_1, outputs=output)
    
    return gan_model

gan_model = define_gan(cnn_model, dense_model)

gan_model.summary()

#%% Define the additional loss function

def slope(predict,lam):
    
    p11 = predict[3]-1
    p12 = -0.5*predict[3]-1
    p21 = predict[4]-1
    p22 = -0.5*predict[4]-1
    p31 = predict[5]-1
    p32 = -0.5*predict[5]-1
    
    S1 = 2*predict[0]/(predict[3]*lam)*(p11*lam**(p11)-p12*lam**(p12));
    S2 = 2*predict[0]/(predict[3]*lam)*(p21*lam**(p21)-p22*lam**(p22));
    S3 = 2*predict[0]/(predict[3]*lam)*(p31*lam**(p31)-p32*lam**(p32));
    
    S  = S1 + S2 + S3
    
    return S

#%% Train the model

import tensorflow.keras.backend as K

def rmse(y_true, y_pred):
    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))

def init_slope(y_true, y_pred):
    
    predict0,predict1,predict2,predict3,predict4,predict5 = y_pred
    true0,   true1,   true2,   true3,   true4,   true5    = y_true
    
    init_pred = predict0 + predict1 + predict2
    init_true = true0 + true1 + true2
    
    value = K.mean(tf.reduce_sum(K.square(y_pred-y_true))) + K.mean(tf.reduce_sum(K.square(init_pred-init_true)))
    
    return value

def init_slope_test(y_true, y_pred):
    
    init_pred = y_pred[0] + y_pred[1] + y_pred[2]
    init_true = y_true[0] + y_true[1] + y_true[2]
    
    # value = K.mean(tf.reduce_sum(K.square(y_pred-y_true))) + K.mean(tf.reduce_sum(K.square(init_pred-init_true)))
    value = tf.sqrt(tf.reduce_sum(tf.pow(tf.subtract(y_true,y_pred),2.0))) + tf.sqrt(tf.reduce_sum(tf.pow(tf.subtract(init_true,init_pred),2.0)))
    
    return value

def slope(y_true, y_pred):
    
    lam = 1.10
    
    # predict0,predict1,predict2,predict3,predict4,predict5 = y_pred
    predict = y_pred
    
    predict0 =  predict[0]
    predict1 =  predict[1]
    predict2 = -predict[2]
    predict3 =  predict[3]
    predict4 = -predict[4]
    predict5 =  predict[5]
    
    p11 = predict3-1
    p12 = -0.5*predict3-1
    p21 = predict4-1
    p22 = -0.5*predict4-1
    p31 = predict5-1
    p32 = -0.5*predict5-1
    
    S1 = 2*predict0/(predict3*lam)*(p11*lam**(p11)-p12*lam**(p12));
    S2 = 2*predict1/(predict4*lam)*(p21*lam**(p21)-p22*lam**(p22));
    S3 = 2*predict2/(predict5*lam)*(p31*lam**(p31)-p32*lam**(p32));
    
    S  = -S1 - S2 - S3
    
    value = K.mean(tf.reduce_sum(K.square(y_pred-y_true))) + K.mean(K.relu(S)/1000)
    
    return value

def keras_custom_loss_function(y_true, y_pred):
    
    return tf.py_function(slope, inp=[y_true, y_pred], Tout=[tf.float32])

def keras_custom_loss_function_1(y_true, y_pred):
    
    return tf.py_function(init_slope, inp=[y_true, y_pred], Tout=[tf.float32])

def keras_custom_loss_function_test(y_true, y_pred):
    
    return tf.py_function(init_slope_test, inp=[y_true, y_pred], Tout=[tf.float32])

#%% Train the model

import tensorflow.keras.backend as K

def rmse(y_true, y_pred):
    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))

opt = tf.keras.optimizers.Adam(learning_rate=1e-3, beta_1=0.9, beta_2=0.999, decay=1e-6)
sgd = tf.keras.optimizers.SGD(lr=1e-3, decay=1e-6, momentum=0.6, nesterov=True)

# Here we use dynamic training process for Ogden Representation
# Training Step 1 (for 500 steps)
gan_model.compile(optimizer=opt, loss='mean_squared_error', metrics='accuracy')
epoch   = 500
# Training Step 2 (for 100 steps)
# gan_model.compile(optimizer=opt, loss=keras_custom_loss_function_test, metrics='accuracy')
# epoch   = 100
# Training Step 3 (for 100 steps)
# gan_model.compile(optimizer=opt, loss='mean_squared_error', metrics='accuracy')
# epoch   = 100

history = gan_model.fit(X_train, Y_train, 
                    batch_size=128, epochs=epoch, steps_per_epoch=40, 
                    validation_data=(X_cv, Y_cv))

predict = gan_model.predict(X_test)

score = gan_model.evaluate(X_test, Y_test, verbose=1)
print('\n', 'Test accuracy', score)

#%% Generating history plots of training

# Summarize history for accuracy
# fig_acc = plt.figure()
# plt.plot(history.history['rmse'])
# plt.plot(history.history['val_rmse'])
# plt.title('model accuracy in training')
# plt.ylabel('accuracy')
# plt.xlabel('epoch')
# plt.legend(['train', 'test'], loc='upper left')
# plt.show()
# fig_acc.savefig('training_accuracy.png')

# fig_acc_log = plt.figure()
# plt.plot(history.history['rmse'])
# plt.plot(history.history['val_rmse'])
# plt.title('model accuracy in training')
# plt.ylabel('accuracy')
# plt.xlabel('epoch')
# plt.yscale('log')
# plt.legend(['train', 'test'], loc='upper left')
# plt.show()
# fig_acc_log.savefig('training_accuracy_log.png')

#%% Summarize history for loss
fig_loss_log = plt.figure()
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.yscale('log')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
fig_loss_log.savefig('training_loss_log.png')

fig_loss = plt.figure()
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
fig_loss.savefig('training_loss.png')

#%% Plot Prediction VS Truth

plt.figure()
plt.plot(Y_test[:,0])
plt.plot(predict[:,0])
plt.title('prediction of first parameter')
plt.legend(['true','predict'])

plt.figure()
plt.plot(Y_test[:,1])
plt.plot(predict[:,1])
plt.title('prediction of second parameter')
plt.legend(['true','predict'])

plt.figure()
plt.plot(Y_test[:,2])
plt.plot(predict[:,2])
plt.title('prediction of third parameter')
plt.legend(['true','predict'])

plt.figure()
plt.plot(Y_test[:,3])
plt.plot(predict[:,3])
plt.title('prediction of fourth parameter')
plt.legend(['true','predict'])

plt.figure()
plt.plot(Y_test[:,4])
plt.plot(predict[:,4])
plt.title('prediction of fifth parameter')
plt.legend(['true','predict'])

plt.figure()
plt.plot(Y_test[:,5])
plt.plot(predict[:,5])
plt.title('prediction of sixth parameter')
plt.legend(['true','predict'])

#%% Visualize the result

art_cst = 0

def func_sca(x, m):
    
    p1 = 2*m[0]/(m[3]-art_cst) * (x**(m[3]-art_cst-1) - x**(-0.5*(m[3]-art_cst)-1))
    p2 = 2*m[1]/m[4] * (x**(m[4]-1) - x**(-0.5*m[4]-1))
    p3 = 2*m[2]/m[5] * (x**(m[5]-1) - x**(-0.5*m[5]-1))
    
    return p1 + p2 + p3

def func_sca_abs(x, m):
    
    m0 = m[0]
    m1 = m[1]
    m2 = m[2]
    m3 = m[3]
    m4 = m[4]
    m5 = m[5]
    
    p1 = 2*m0/m3 * (x**(m3-1) - x**(-0.5*m3-1))
    p2 = 2*m1/m4 * (x**(m4-1) - x**(-0.5*m4-1))
    p3 = 2*m2/m5 * (x**(m5-1) - x**(-0.5*m5-1))
    
    return p1 + p2 + p3

# Take test set 1

test_index = 0

stress_pred = func_sca_abs(lambda_set, predict[test_index])
stress_real = func_sca_abs(lambda_set, Y_test[test_index])

plt.figure()
plt.plot(strain_set, stress_pred, 'r-')
plt.plot(strain_set, stress_real, 'b-')
plt.legend(['Predicted Curve','True Curve'])
plt.title('Validation on Test set 1')

# Take test set 2

test_index = 20

stress_pred = func_sca_abs(lambda_set, predict[test_index])
stress_real = func_sca_abs(lambda_set, Y_test[test_index])

plt.figure()
plt.plot(strain_set, stress_pred, 'r-')
plt.plot(strain_set, stress_real, 'b-')
plt.legend(['Predicted Curve','True Curve'])
plt.title('Validation on Test set 2')

# Take test set 3

test_index = 200

stress_pred = func_sca_abs(lambda_set, predict[test_index])
stress_real = func_sca_abs(lambda_set, Y_test[test_index])

plt.figure()
plt.plot(strain_set, stress_pred, 'r-')
plt.plot(strain_set, stress_real, 'b-')
plt.legend(['Predicted Curve','True Curve'])
plt.title('Validation on Test set 3')

#%% Evaluate the Norm Error Based on Curve

[p1, p2] = predict.shape

error1 = np.zeros((p1))
error2 = np.zeros((p1))
error3 = np.zeros((p1))
error4 = np.zeros((p1))
error5 = np.zeros((p1))
error6 = np.zeros((p1))

for i in range(p1):
    error1[i] = np.abs(predict[i][0] - Y_test[i][0])
    error2[i] = np.abs(predict[i][1] - Y_test[i][1])
    error3[i] = np.abs(predict[i][2] - Y_test[i][2])
    error4[i] = np.abs(predict[i][3] - Y_test[i][3])
    error5[i] = np.abs(predict[i][4] - Y_test[i][4])
    error6[i] = np.abs(predict[i][5] - Y_test[i][5])
    
error1_ave = np.mean(error1)
error2_ave = np.mean(error2)
error3_ave = np.mean(error3)
error4_ave = np.mean(error4)
error5_ave = np.mean(error5)
error6_ave = np.mean(error6)

print('error_1_ave ', error1_ave)
print('error_2_ave ', error2_ave)
print('error_3_ave ', error3_ave)
print('error_4_ave ', error4_ave)
print('error_5_ave ', error5_ave)
print('error_6_ave ', error6_ave)

#%% Evaluate the Norm Error Based on Curve

[p1,p2] = predict.shape

error = np.zeros((p1))
error_max = np.zeros((p1))

for i in range(p1):
    
    stress_pred  = func_sca_abs(lambda_set, predict[i,:])
    stress_real  = func_sca_abs(lambda_set, Y_test[i,:])
    error[i]     = np.linalg.norm(stress_pred-stress_real)
    error_max[i] = np.max(np.abs(stress_pred-stress_real))
       
print("Norm-2 Error based on ss curve is:", np.mean(error))
print("Norm-2 Error based on ss curve is:", np.max(error))

print("Absolute Error based on ss curve is:", np.mean(error_max))
print("Absolute Error based on ss curve is:", np.max(error_max))

#%%

#gan_model.save('trained_ogden_128.h5')

#%% Evaluate the Error of Decoder Network Again

endpoint_model = tf.keras.models.load_model('trained_dense_end_point_128.h5')
# endpoint_model = tf.keras.models.load_model('trained_end_point_128.h5')

#%% Get the Latent feature prediction from the feature extraction module
pred_mid = cnn_model(X_test)
pred_mid = tf.squeeze(pred_mid)

#%% Compare the predicted endpoint stress from only predicting endpoint stress or predicting endpoint stress as well as parameters (turns out there is no significant difference)
pred_end = endpoint_model(pred_mid)
pred_end = gan_model(X_test)

#%% Let's try a second modification method

# Here define the clip (clip value defines where to stop the modification)
clip = 1.00 # This means there is no clip (as our stretch value is from 1 to 1.15)

# Define the modification function with clip (since we find out there is no need to clip, so this function can be ignored)
def quadratic(x, w):
    
    # clip = 1.08
    p  = (x>clip)*w*(x-clip)**2
    
    return p

# Calculate the end point stress value (This is the key part for developing the Modification Function)
def Ogden_func_end(x, w):
    
    p1 = 2*w[0]/w[3] * (x**(w[3]-1) - x**(-0.5*w[3]-1))
    p2 = 2*w[1]/w[4] * (x**(w[4]-1) - x**(-0.5*w[4]-1))
    p3 = 2*w[2]/w[5] * (x**(w[5]-1) - x**(-0.5*w[5]-1))
    
    return p1 + p2 + p3

#%% New implement the modification module
predict_new = np.zeros((p1,p2))
q1 = len(lambda_set)

end_value = np.zeros((p1,1)) 
delta_y   = np.zeros((p1,1))
curve_gap = np.zeros((p1,q1)) # Keep track of the error as strain (stretch) increases

for i in range(p1):
    
    end_value[i] = Ogden_func_end(1.15,predict[i][0:6])
    
    delta_y[i]   = (end_value[i] - predict[i][6]) / ((1.15-clip)**2)
    # curve_gap[i,:]    = quadratic(lambda_set, delta_y)
    
# Now Re-evaluate the results after adding the modified modules
error_new_1 = np.zeros((p1))
error_new_2 = np.zeros((p1))

for i in range(p1):
    
    stress_pred      = func_sca(lambda_set, predict[i])
    stress_pred_new  = func_sca(lambda_set, predict[i]) - quadratic(lambda_set, delta_y[i])
    stress_real      = func_sca(lambda_set, Y_test[i])
    error_new_1[i]   = np.linalg.norm(stress_pred-stress_real)
    error_new_2[i]   = np.linalg.norm(stress_pred_new-stress_real)   
    
print("Norm-2 Error based on simple ss curve is:", np.mean(error_new_1))
print("Norm-2 Error based on simple ss curve is:", np.max(error_new_1))

print("Norm-2 Error based on modified ss curve is:", np.mean(error_new_2))
print("Norm-2 Error based on modified ss curve is:", np.max(error_new_2))

#%% Now Validate and Visualize how the result changes when adding the modification module

# Take test set 1
test_index = 120

stress_pred     = func_sca(lambda_set, predict[test_index])
stress_pred_new = func_sca(lambda_set, predict[test_index]) - quadratic(lambda_set, delta_y[test_index])
stress_real     = func_sca(lambda_set, Y_test[test_index])

plt.figure()
plt.plot(strain_set, stress_real, 'b-')
plt.plot(strain_set, stress_pred, 'g-')
plt.plot(strain_set, stress_pred_new, 'r-')

plt.legend(['Original Curve','Baseline-2 Predicted Curve','PBNN Predicted Curve'])
plt.title('Prediction Validation on test set sample 2', fontsize=14)
#plt.grid()
plt.xlabel('Strain', fontsize=14)
plt.ylabel('Stress / MPa', fontsize=14)

#%% Take test set 2
test_index = 20

stress_pred = func_sca(lambda_set, predict[test_index])
stress_pred_new = func_sca(lambda_set, predict[test_index]) - quadratic(lambda_set, delta_y[test_index])
stress_real = func_sca(lambda_set, Y_test[test_index])

plt.figure()
plt.plot(strain_set, stress_real, 'b-')
plt.plot(strain_set, stress_pred, 'g-')
plt.plot(strain_set, stress_pred_new, 'r-')

plt.legend(['Original Curve','Baseline-2 Predicted Curve','PBNN Predicted Curve'])
plt.title('Prediction Validation on test set sample 1', fontsize=14)
plt.grid()
plt.xlabel('Strain', fontsize=14)
plt.ylabel('Stress / MPa', fontsize=14)

#%% Take test set 3
test_index = 300

stress_pred = func_sca(lambda_set, predict[test_index])
stress_pred_new = func_sca(lambda_set, predict[test_index]) - quadratic(lambda_set, delta_y[test_index])
stress_real = func_sca(lambda_set, Y_test[test_index])

plt.figure()
plt.plot(strain_set, stress_real, 'b-')
plt.plot(strain_set, stress_pred, 'g-')
plt.plot(strain_set, stress_pred_new, 'r-')

plt.legend(['Original Curve','Baseline-2 Predicted Curve','PBNN Predicted Curve'])
plt.title('Prediction Validation on test set sample 1', fontsize=14)
plt.grid()
plt.xlabel('Strain', fontsize=14)
plt.ylabel('Stress / MPa', fontsize=14)

#%% Plot the coefficients
plt.figure()
plt.plot(ss_set)
plt.title('Ogden paramter values for different models',fontsize=14)
plt.xlabel('model#',fontsize=14)
plt.ylabel('parameter value',fontsize=14)
plt.legend([r'$\mu_1$',r'$\mu_2$',r'$\mu_3$',r'$\alpha_1$',r'$\alpha_2$',r'$\alpha_3$'])
